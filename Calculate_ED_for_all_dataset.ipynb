{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XU5-zf5Ad3k3"
   },
   "source": [
    "After observing the yahoo finance package, found some days' closing price are not correct. So, the extra script help to fix the data. There is no need to download and fix manully everytime. I already email to the yahoo finance to fix this data. If they fix, then there will be no use of this first script in future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xi5GaAFCD3eJ",
    "outputId": "e6b734e5-10f2-4325-b018-8d777b1af5b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading EUR/USD data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34990/2381738963.py:8: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(ticker, start=\"2000-01-01\", progress=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattening columns...\n",
      "\n",
      "Applying corrections...\n",
      "Fixing 2008-01-08: Old=1.5571 -> New=1.4705\n",
      "Fixing 2008-02-08: Old=1.5571 -> New=1.4503\n",
      "Fixing 2008-08-08: Old=1.5049 -> New=1.5074\n",
      "Fixing 2008-09-08: Old=1.5050 -> New=1.4250\n",
      "Fixing 2008-10-08: Old=1.4957 -> New=1.3650\n",
      "Fixing 2008-12-08: Old=1.4918 -> New=1.2930\n",
      "\n",
      "Done. Saved to EURUSD_Close_Fixed.csv\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def download_and_fix_eurusd_close():\n",
    "    # Download EUR/USD data\n",
    "    print(\"Downloading EUR/USD data...\")\n",
    "    ticker = \"EURUSD=X\"\n",
    "    df = yf.download(ticker, start=\"2000-01-01\", progress=False)\n",
    "\n",
    "    df = df[['Close']].copy()\n",
    "\n",
    "    # Flatten columns if multi-index (common in yfinance)\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        print(\"Flattening columns...\")\n",
    "        df.columns = df.columns.get_level_values(0)\n",
    "\n",
    "    # Taken from https://www.kaggle.com/datasets/lehenzehra/eurusd-daily-data-ohlc?select=EURUSD_D1_Sorted.csv\n",
    "    corrections = {\n",
    "        \"2008-01-08\": 1.4705,\n",
    "        \"2008-02-08\": 1.4503,\n",
    "        \"2008-08-08\": 1.5074,\n",
    "        \"2008-09-08\": 1.4250,\n",
    "        \"2008-10-08\": 1.3650,\n",
    "        \"2008-12-08\": 1.2930,\n",
    "    }\n",
    "\n",
    "    print(\"\\nApplying corrections...\")\n",
    "\n",
    "    # Apply corrections\n",
    "    for date_str, price in corrections.items():\n",
    "        dt = pd.Timestamp(date_str)\n",
    "        if dt in df.index:\n",
    "            print(f\"Fixing {date_str}: Old={df.at[dt, 'Close']:.4f} -> New={price:.4f}\")\n",
    "            df.at[dt, 'Close'] = price\n",
    "        else:\n",
    "            print(f\"Warning: {date_str} not found in data.\")\n",
    "\n",
    "\n",
    "    # Save to CSV\n",
    "    output_file = \"EURUSD_Close_Fixed.csv\"\n",
    "    df.to_csv(output_file)\n",
    "    print(f\"\\nDone. Saved to {output_file}\")\n",
    "\n",
    "\n",
    "download_and_fix_eurusd_close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKtUL89vecY7"
   },
   "source": [
    "Load the corrected data and make a 'difference' percentafge column based on the closing price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2vHKLCk2Px_2"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"EURUSD_Close_Fixed.csv\", index_col=0, parse_dates=True)\n",
    "df['difference'] = df['Close'].pct_change() * 100\n",
    "df.head()\n",
    "df.to_csv('EURUSD_Close_Fixed_with_difference.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtUAKNsQeqt4"
   },
   "source": [
    "The main script started from here. Creating main data table for calculating Euclidean Distane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82ea1b91",
    "outputId": "f4b3b8d2-74a4-44b1-f38c-9bcb8f870e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 'main_data_table.csv'.\n"
     ]
    }
   ],
   "source": [
    "data = df['difference'].dropna().tolist()\n",
    "\n",
    "main_data_rows = []\n",
    "window_size = 6\n",
    "\n",
    "# Iterate through the data to create feature vectors and target values\n",
    "for i in range(len(data) - window_size + 1):\n",
    "    row = data[i : i + window_size]\n",
    "    feature_vector = row[:5]\n",
    "    true_value = row[5]\n",
    "    main_data_rows.append(feature_vector + [true_value])\n",
    "\n",
    "# Create column names for the new DataFrame\n",
    "column_names = [f'p{j+1}' for j in range(5)] + ['true_value_next_day']\n",
    "\n",
    "# Create the 'main_data' DataFrame\n",
    "main_data = pd.DataFrame(main_data_rows, columns=column_names)\n",
    "\n",
    "# Set the DataFrame index to start from 1\n",
    "main_data.index = range(1, len(main_data) + 1)\n",
    "\n",
    "#Saving \"Main Data Table\"\n",
    "main_data.to_csv('main_data_table.csv', index_label='Vectors')\n",
    "print(\"Saved 'main_data_table.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoz5ePjde70h"
   },
   "source": [
    "After forming the \"difference\" column as vectors, calcultaing the Euclidean Distance. And do statics of the positive and negative outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aoqDsVKMXsCf",
    "outputId": "3874bc56-ae47-4018-ab56-fce85c9083c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at => 2025-12-27 00:19:41.350156\n",
      "\n",
      "-----Calculating Euclidean Distance-----\n",
      "\n",
      "T: --> 0.5--> 0.6--> 0.7--> 0.8--> 0.9--> 1.0--> 1.1--> 1.2--> 1.3--> 1.4--> 1.5\n",
      "Results Different THRESHOLDS:\n",
      "Finished at => 2025-12-27 00:22:24.305883\n",
      "Total time take => 0:02:42.955727\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "#Create base_vector range -2 to 2, each time take 5 datapoints for a vectors.\n",
    "generated_base_vectors = np.random.uniform(low=-2.0, high=2.0, size=(5, 5))\n",
    "\n",
    "\n",
    "final_results = []\n",
    "start_time = datetime.now()\n",
    "print(f\"Started at => {start_time}\")\n",
    "print(f\"\\n-----Calculating Euclidean Distance-----\\n\")\n",
    "print(\"T: \", end=\"\")\n",
    "for current_threshold in np.arange(0.5, 1.51, 0.1):\n",
    "    print(f\"--> {current_threshold:.1f}\", end=\"\")\n",
    "    accepted_results_list = []\n",
    "\n",
    "    # Iterate through each generated base vector\n",
    "    for gen_base_vec_idx in range(generated_base_vectors.shape[0]):\n",
    "        current_base_vector = generated_base_vectors[gen_base_vec_idx]\n",
    "        base_vectors_for_each_threshold = []\n",
    "\n",
    "        for index, row in main_data.iterrows():\n",
    "            feature_vector = np.array(row[['p1', 'p2', 'p3', 'p4', 'p5']].tolist())\n",
    "            euclidean_distance = np.linalg.norm(feature_vector - current_base_vector)\n",
    "            v_target = 0;\n",
    "\n",
    "            if euclidean_distance < current_threshold:\n",
    "                v_target = row['true_value_next_day']\n",
    "                if current_base_vector.tolist() not in base_vectors_for_each_threshold:\n",
    "                    base_vectors_for_each_threshold.append(current_base_vector.tolist())\n",
    "\n",
    "                accepted_results_list.append({\n",
    "                    'r1': feature_vector.tolist(),\n",
    "                    'v_target': v_target,\n",
    "              })\n",
    "\n",
    "        if accepted_results_list and base_vectors_for_each_threshold: # Check if the list is not empty\n",
    "            accepted_results_df = pd.DataFrame(accepted_results_list)\n",
    "            positive_count = (accepted_results_df['v_target'] > 0).sum()\n",
    "            negative_count = (accepted_results_df['v_target'] <= 0).sum()\n",
    "            positive_percentage=(positive_count/len(accepted_results_df))*100\n",
    "            negative_percentage=(negative_count/len(accepted_results_df))*100\n",
    "            r1_values = accepted_results_df['r1']\n",
    "            v_target_values = accepted_results_df['v_target']\n",
    "            \n",
    "            final_results.append({\n",
    "                'r1': r1_values.tolist(),\n",
    "                'v_target': v_target_values.tolist(),\n",
    "                'B': base_vectors_for_each_threshold,\n",
    "                'threshold': f\"{current_threshold:.1f}\",\n",
    "                '% positive_Vtarget': f\"{positive_percentage:.2f}\",\n",
    "                '% negative_Vtarget': f\"{negative_percentage:.2f}\"\n",
    "            })\n",
    "\n",
    "final_result_df = pd.DataFrame(final_results)\n",
    "print(\"\\nResults Different THRESHOLDS:\")\n",
    "final_result_df.to_csv('final_result_df.csv', index= True)\n",
    "end_time = datetime.now()\n",
    "print(f\"Finished at => {end_time}\")\n",
    "print(f\"Total time take => {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "malformed node or string: [0.3564305352885766, 0.3739182328805768]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mast\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m final_result_df[\u001b[33m'\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mfinal_result_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mv_target\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mliteral_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m final_result_df[[\u001b[33m'\u001b[39m\u001b[33mv_target\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m'\u001b[39m]].head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-project/ANN-for-Time-Series-Forex-PyNote/venv/lib/python3.12/site-packages/pandas/core/series.py:4943\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4810\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4815\u001b[39m     **kwargs,\n\u001b[32m   4816\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4817\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4818\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4819\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4934\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4935\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4937\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4941\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-project/ANN-for-Time-Series-Forex-PyNote/venv/lib/python3.12/site-packages/pandas/core/apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-project/ANN-for-Time-Series-Forex-PyNote/venv/lib/python3.12/site-packages/pandas/core/apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-project/ANN-for-Time-Series-Forex-PyNote/venv/lib/python3.12/site-packages/pandas/core/base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-project/ANN-for-Time-Series-Forex-PyNote/venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mast\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m final_result_df[\u001b[33m'\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m'\u001b[39m] = final_result_df[\u001b[33m'\u001b[39m\u001b[33mv_target\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(\u001b[43mast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mliteral_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m      3\u001b[39m final_result_df[[\u001b[33m'\u001b[39m\u001b[33mv_target\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m'\u001b[39m]].head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ast.py:112\u001b[39m, in \u001b[36mliteral_eval\u001b[39m\u001b[34m(node_or_string)\u001b[39m\n\u001b[32m    110\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m left - right\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_signed_num(node)\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_or_string\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ast.py:111\u001b[39m, in \u001b[36mliteral_eval.<locals>._convert\u001b[39m\u001b[34m(node)\u001b[39m\n\u001b[32m    109\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    110\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m left - right\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_signed_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ast.py:85\u001b[39m, in \u001b[36mliteral_eval.<locals>._convert_signed_num\u001b[39m\u001b[34m(node)\u001b[39m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     84\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m - operand\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ast.py:76\u001b[39m, in \u001b[36mliteral_eval.<locals>._convert_num\u001b[39m\u001b[34m(node)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_convert_num\u001b[39m(node):\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Constant) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(node.value) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mcomplex\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m         \u001b[43m_raise_malformed_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m node.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ast.py:73\u001b[39m, in \u001b[36mliteral_eval.<locals>._raise_malformed_node\u001b[39m\u001b[34m(node)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lno := \u001b[38;5;28mgetattr\u001b[39m(node, \u001b[33m'\u001b[39m\u001b[33mlineno\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     72\u001b[39m     msg += \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m on line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlno\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg + \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: malformed node or string: [0.3564305352885766, 0.3739182328805768]"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "final_result_df['count'] = final_result_df['v_target'].apply(lambda x: len(ast.literal_eval(x)))\n",
    "final_result_df[['v_target', 'count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
